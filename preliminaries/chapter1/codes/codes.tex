\documentclass[../../../main]{subfiles}

\begin{document}
% subsection %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Linear Codes}

In elementary physics, we learn of the principal of superposition while studying oscillation theory; we learn how energy passing through a medium in the form of a wave can {\it interfere} with other such waves to create a new wave, the sum of the waves. This persists until these bursts of energy have passed.

The modern theories of commumincation and information seak to pass information via electrical signals (waves) passing through some channel (media). As information in the form of signals passes through the channel, it will invariably come into contact with other signals, whether latent or otherwise. The signal will then change, and there is no guaruntee that the signal received will be the signal sent; whence, one of the fundamental problems of digital communication is exposed. It becomes necessary, then, to attempt a solution. \cite{hill-coding} provides the following instructive example.

Imagine there are two possible messages that we would like to send: {\tt YES} and {\tt NO}. And suppose further that {\tt YES} is encoded as {\tt 0} and {\tt NO} as {\tt 1}. If we had sent the message {\tt 0}, there is the possibility of the single bit being flipped, i.e. {\tt 1} is received instead of {\tt 0}, the initial message. The receiver would then have an incorrect message. It isn't difficult to envisage senarios in which such an error would have a terrible impact.

Instead, we will encode {\tt YES} as {\tt 00000} and {\tt NO} as {\tt 11111}. If we sent {\tt 00000} through the channel, and if the receiver got {\tt 01000}, then it would be reasonable to assume\Anote{nearest-neighbour} that the message intended was {\tt YES}, a correct assumption. Since a single bit was all that was necessary to convey our minimal lexicon \{{\tt YES, NO}\}, we see that the remaining 4 bits are {\it redundant}; however, we also see that these redundancies allowed us to correct the message in the case that any single bit was flipped. Similarly, if any two bits had been flipped, then we also would have correctly interpreted the message. If, however, we sent {\tt 00000} and received {\tt 00111}, then we would have interpreted this message as {\tt 11111}, clearly incorrect. So we see that there is a limitation of our ability to correct errors in transmition, but this is hardely unexpected. In any event, the ability to detect and correct errors is a remarkable property whose importance cannot be overstated in today's digital world.

The process of appending to {\tt 0} and {\tt 1} the redundant strings of bits {\tt 0000} and {\tt 1111}, respectively, is called {\it encoding}, and we now introduce such a method.

Suppose we had the binary string $x=x_0 x_1 \cdots x_{n-1}$, where the first $k$ bits $x_0 x_1 \cdots x_{k-1}$ are the message we desire to preserve upon sending through some channel. We choose the remaining {\it check bits} $x_k x_{k+1} \cdots x_{n-1}$ (the redundancies) in such a way that
\[
Hx^t = 0 \qquad (\text{mod } 2),
\]
where $H$ is the binary $(n-k) \times n$ matrix called the {\it parity check matrix} of the code. Moreover, by our assumptions on $x$, $H$ can be assumed to have the form
\[
H=(A \mid I_{n-k}),
\]
for some binary matrix $A$. 

More generally, we may take $x$ and $H$ to be over any field $\gf(q)$. We have the following definition.

\begin{defin}\label{linear-code-defin}\index{linear error-correcting code}
 Let $H=(A \mid I_{n-k})$ be an $(n-k) \times n$ matrix over $\gf(q)$. The {\it linear code} $\code$ with parity check matrix $H$ is given by $\code = \text{Null}(H) = \{x \in \gf(q^n) : Hx^t = 0\}$, where the extension $\gf(q^n)$ is interpreted here as a linear space over $\gf(q)$. We say that $\code$ is a linear $[n,k]_q$-code, where clearly $\text{dim}(\code) = k$.
\end{defin}

\begin{ex}
 The repitition code over $\gf(q) = \{0,a_1,\dots,a_{q-1}\}$ of length $n$ is given by
 \begin{equation}
 \arraycolsep=1.25pt\def\arraystretch{0.625}
  \overbrace{
  \begin{array}{cccc}
  \arraycolsep=1.0pt\def\arraystretch{1.0}
   0 & 0 & \cdots & 0 \\
   a_1 & a_1 & \cdots & a_1 \\
   \vdots & \vdots & & \vdots \\
   a_{q-1} & a_{q-1} & \cdots & a_{q-1}
  \end{array}
  }^n
 \end{equation}
 and has parity check matrix $H = (\jj_{n-1} \mid I_{n-1})$.
\end{ex}

We have defined a linear code by its parity check matrix. An equivalent definition is to use the so-called {\it generator matrix} $G$. If $H=(A \mid I_{n-k})$ is the parity check matrix of the code, then $G=(I_k \mid -A^t)$. The code is then given by the linear span of the rows of $G$. Note that by construction, it follows that $GH^t = HG^t = O$.

\begin{ex}
 The repitition code of the previous example has the generator matrix $\jj_n^t$.
\end{ex}

The dual of a linear code $\code$ is given in the usual way by $\code^\perp = \{x \in \gf(q^n) : xy^t=0 \text{, for every }y\in\code\}$\index{linear error-correcting code!dual of}. If $H$ and $G$ are the parity check and generator matrices of $\code$, then $G$ and $H$ are the parity check and generator matrices of $\code^\perp$, respectively.

In Definition \ref{linear-code-defin}, there are two parameters explicitly given of a code, namely, the length $n$ of the codewords and the dimension $k$ of the linear space consisting of the codewords. Two more fundamental parameters for us are the minimum distance and the minimum weight defined thus.

\begin{defin}\label{wt-dist}\index{Hamming weight}\index{Hamming distance}
 Let $\code$ be a linear $[n,k]_q$-code. Then the {\it Hamming weight} of a codeword $x=x_0 x_1 \cdots x_{n-1} \in \code$ is given by 
 \begin{defenum}
 \item $\wt(x) = \#\{i : x_i \neq 0\}$. 
 \end{defenum}
 If $y=y_0 y_1 \cdots y_{n-1} \in \code$ is any other codeword, then the {\it Hamming distance} between $x$ and $y$ is defined as 
 \begin{defenum}[resume]
 \item $\dist(x,y) = \#\{i : x_i \neq y_i\}$. 
 \end{defenum}
 We then have that the minimum weight of the code and the minimum distance of the code are 
 \begin{defenum}[resume]
 \item $\wt(\code) = \min_{x \in \code\bb\{0\}} \wt(x)$ and
 \item $\dist(\code) = \min_{\begin{smallmatrix}x,y\in\code\\ x \neq y\end{smallmatrix}}\dist(x,y)$. 
 \end{defenum}
 If we wish to emphasize the distance $d=\dist(\code)$ of a code, we write $[n,k,d]_q$-code; and to emphasize the weight $w=\wt(\code)$ of a code as well, $[n,k,d,w]_q$-code.
\end{defin}

The Hamming distance can easily be seen to form a metric\Anote{metric} on $\gf(q^n)$. The next result is then clear \cite[see][Theorem 1.9]{hill-coding}.

\begin{prop}
 If $\code$ is any $[n,k,d]_q$-code, then the following hold.
 \begin{defenum}
  \item The code can detect $s$ errors if $\dist(\code) \geq s+1$;
  \item\label{distance} the code can correct up to $t$ errors if $\dist(\code) \geq 2t+1$; and
  \item\label{wt-dist-eq} for every $x,y \in \code$, it holds that $\dist(x,y) = \wt(x-y)$, hence $\dist(\code)=\wt(\code)$.
 \end{defenum}
\end{prop}

We note that in the case the code is nonlinear, the condition \ref{wt-dist-eq} fails to hold in general.

We give one final result in this subsection relating the parity check matrix and the minimum distance.

\begin{prop}\label{dist-parity}
 Let $\C$ be a linear $[n,k]_q$-code with parity check matrix $H$. Then $\code$ has $\dist(\code) \geq d$ if and only if every $d-1$ columns of $H$ are linearly independent.
\end{prop}

\begin{proof}
 To show necessity, let $x \in \code$ and $w = \wt(x)$. Since $Hx^t=0$, $H$ has at least $w$ linearly dependent columns; hence, if if any $d-1$ columns of $H$ are linearly independent, then, by an application of \ref{wt-dist-eq}, $\code$ cannot have a codeword of weight $d-1$ or less. 
 
 Towards sufficiency, assume that $H$ has $d-1$ linearly independent columns. If $c_0, \dots, c_{n-1}$ are the columns of $H$, then there is a dependence relation $a_0c_0 + \cdots + a_{n-1}c_{n-1} = 0$ ($a_0, \dots, a_{n-1} \in \gf(q)$) for which at most $d-1$ of the $a_i$s are nonzero. Then the codeword $x=a_0a_2 \cdots a_{n-1}$ has weight less than $d$, thereupon $\dist(\code) \leq d$. 
\end{proof}

\dinkus

% subsection %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Hamming and Simplex Codes}

The theory of error-correcting codes began with the seminal paper of Richard W. Hamming \citeyearpar{richard-hamming} who introduced redundancy for the purpose of error correction. In this paper, he also constructed a most useful family of codes, aptly called the {\it Hamming codes}. The codes constructed by Hamming were binary; however, it is a simple matter to extend the construction to an arbitrary finite field.

\begin{defin}\label{hamming-code}\index{Hamming code}\index{simplex code}\label{hamming-simplex-def}
 Let $H$ be the $n \times (q^n-1)/(q-1)$ matrix over $\gf(q)$ whose columns are representatives of the nonzero 1-dimensional subspaces of the extension field $\gf(q^n)$ over $\gf(q)$. The linear code $\hamming_{q,n}$ whose parity check matrix is $H$ is called a {\it Hamming code}. The linear code $\simplex_{q,n}$ whose generator matrix is $H$ is called a {\it simplex code}.
\end{defin}

\begin{ex}
The following is a simplex code in $\gf(9)$ over $\gf(3) \simeq \Z/3\Z$.
\begin{small}
\begin{equation}
\arraycolsep=1.25pt\def\arraystretch{0.625}
 \left(\begin{array}{cccc}
\bar{1}&\bar{1}&\bar{2}&\bar{0}\\
\bar{2}&\bar{1}&\bar{0}&\bar{2}\\
\bar{0}&\bar{1}&\bar{1}&\bar{1}\\
\bar{1}&\bar{2}&\bar{0}&\bar{1}\\
\bar{2}&\bar{2}&\bar{1}&\bar{0}\\
\bar{0}&\bar{2}&\bar{2}&\bar{2}\\
\bar{1}&\bar{0}&\bar{1}&\bar{2}\\
\bar{2}&\bar{0}&\bar{2}&\bar{1}\\
\bar{0}&\bar{0}&\bar{0}&\bar{0}\\
 \end{array}\right).
\end{equation}
\end{small}
\end{ex}

The following result is immediate.

\begin{prop}
 Let $q$ be a prime power, $n > 1$, and let $v=(q^n-1)/(q-1)$. Then:
 \begin{defenum}
  \item $\simplex_{q,n}$ is a linear $[v,n]_q$-code, and
  \item $\hamming_{q,n}$ is a linear $[v,v-n]_q$-code.
 \end{defenum}
\end{prop}

Using Proposition \ref{dist-parity}, we see that $\hamming_{q,n}$ is a linear single-error correcting code. In what follows, however, the simplex code will play a central role. Due to it's importance, then, we work through the rather lengthy proof of the following characterizing result \cite[see][Theorem 3.9.27]{combinatorics-of-symmetric-designs}.

\begin{thm}\label{simplex-properties}
 Let $q$ be a prime power, $n>1$, and let $v=(q^n-1)/(q-1)$. Then a linear $[v,n]_q$-code $\code$ is a $\simplex_{q,n}$ code if and only if $\wt(x)=q^{n-1}$, for every $x \in \code$.
\end{thm}

\begin{proof}
 Let $\code$ be a linear $[v,n]$-code in $\gf(q^v)$, and let $W=\left( \begin{smallmatrix} x_0 \\\hline \vdots \\\hline x_{v-1} \end{smallmatrix} \right)$, where the $x_i$ are representatives of the nonzero 1-dimensional subspaces of $\code$. Without loss of generality, assume that the first $n$ rows of $W$ form a generator matrix $H$ for the code $\code$. Let $y_0, \dots, y_{v-1}$ be the columns of $W$ and let $Y = \langle y_0, \dots, y_{v-1} \rangle$. Then $\mathrm{dim}(Y)=\mathrm{rank}(W)=\mathrm{dim}(\code)=n$. For $i \in \{0,\dots, v-1\}$, let $U_i$ be the hyperplane generated by all strings with the $i$-th position 0.
 
 If $\code$ is a $\simplex_{q,n}$, then the columns of $H$ are representatives of all the distinct 1-dimensional subspaces of a linear $n$-dimensional space over $\gf(q)$. By extension, then, $y_0, \dots, y_{v-1}$ are representatives of the distinct 1-dimensional subspaces of $Y$. If $Y \subseteq U_i$, then $x_i=0$, a contradiction. Therefore, $\mathrm{dim}(Y \cap U_i) = n-1$ so that $Y \cap U_i$ has $(q^{n-1}-1)/(q-1)$ 1-dimensional subspaces. It follows that $\wt(x_i) = v - (q^{n-1}-1)/(q-1) = q^{n-1}$, but every string in $\code$ is a scalar multiple of some $x_i$, hence $\wt(x) = q^{n-1}$, for every $x \in \code$.
 
 Towards necessity, assume that $\wt(x)=q^{n-1}$, for every $x \in \code$. As the code $\code$ is linear, it follows that it is equidistant with $d(\code)=q^{n-1}$. 
 
 If $Y \cap U_i = Y \cap U_j$, then $x_i$ and $x_j$ have zeros in the same $(q^{n-1}-1)/(q-1)$ positions. Then, if $W_{ih} \neq 0$, $W_{jh} \neq 0$ and $W_{ih}=\alpha W_{jh}$ for some $\alpha \in \gf(q)^*$. It follows that
 \[
 \dist(x_i, \alpha x_j) \leq v - \frac{q^{n-1}-1}{q-1} -1 < q^{n-1},
 \]
 a contradiction. Since $x_i \neq \alpha x_j$, we have $Y \cap U_i \neq Y \cap U_j$. Then $\mathrm{Y \cap U_i \cap U_j} = n-2$ so that there are $(q^{n-2}-1)/(q-1)$ indices $h$ for which $W_{ih} = W_{jh} = 0$. If $N$ is the matrix obtained from $W$ by replacing the nonzero entries with 1, then our discussion shows that two distinct rows of $N$ have inner product 
 \[
 v - 2\frac{q^{n-1}-1}{q-1} + \frac{q^{n-2}-1}{q-1} = q^{n-1}-q^{n-2};
 \]
 hence, $N$ is the incidence matrix of a square $\bibd(v,q^{n-1},q^{n-1}-q^{n-2})$. By Proposition \ref{square-properties}, no two columns of $N$ are proportional, thereupon no two columns of $W$ are proportional.
 
 It remains to show that the columns of $H$ represent all the distinct 1-dimensional subspaces of an $n$-dimensional linear space over $\gf(q)$. Let $z_0, \dots, z_{v-1}$ be the columns of $H$. Suppose there are distinct $i,j \in \{0,\dots, v-1\}$ and an $\alpha \in \gf(q)^*$ such that $z_i=\alpha z_j$. Then $W_{hi}=\alpha W_{hj}$, for all $h \in \{0, \dots, n-1\}$. Since $\{x_0, \dots, x_{n-1}\}$ is a basis for $\code$, it follows that there are $\beta_0, \dots, \beta_{n-1} \in \gf(q)$ such that $x_\ell = \ssum_k \beta_kx_k$. Thus,
 \[
 W_{\ell i} = \sum_k \beta_k W_{ki} = \alpha\sum_k \beta_k W_{kj} = \alpha W_{\ell j},
 \]
 from which it would follow that $y_i = \alpha y_j$, which cannot happen. Then $H$ is a generator matrix of $\code$, and $\code$ is a $\simplex_{q,n}$.
\end{proof}

The fact that $\simplex_{q,n}$ is constant weight---and hence equidistant---will be used to great effect in Part II of this essay.
 
\biblio
\end{document}
